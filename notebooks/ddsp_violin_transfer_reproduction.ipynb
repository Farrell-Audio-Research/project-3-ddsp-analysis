{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "collapsed": true,
        "id": "T_qlTvmCUkRk",
        "outputId": "2afd6804-8e73-4cca-8454-b059af4a1463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ddsp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4114957747.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mddsp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ddsp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# CELL 1 — runtime check (no installs on server)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import sys, os, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import ddsp, ddsp.training\n",
        "import librosa, librosa.display\n",
        "import soundfile as sf\n",
        "\n",
        "# Quiet TF info/warnings if you like:\n",
        "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"TF:\", tf.__version__, \"| DDSP:\", ddsp.__version__)\n",
        "print(\"librosa:\", librosa.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2 #\n",
        "# ---- Light replacements for colab_utils ----\n",
        "DEFAULT_SAMPLE_RATE = 16000\n",
        "\n",
        "def play(y, sr=DEFAULT_SAMPLE_RATE):\n",
        "    \"\"\"Inline audio player for Jupyter (no google.colab).\"\"\"\n",
        "    from IPython.display import Audio, display\n",
        "    if y.ndim == 2 and y.shape[0] == 1:\n",
        "        y = y[0]\n",
        "    display(Audio(y, rate=sr))\n",
        "\n",
        "def specplot(audio, sr=DEFAULT_SAMPLE_RATE, title=None):\n",
        "    \"\"\"Log-magnitude spectrogram plot.\"\"\"\n",
        "    if audio.ndim == 2 and audio.shape[0] == 1:\n",
        "        audio = audio[0]\n",
        "    S = np.abs(librosa.stft(audio, n_fft=1024, hop_length=256))**2\n",
        "    Sdb = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(8, 3))\n",
        "    librosa.display.specshow(Sdb, sr=sr, hop_length=256, x_axis='time', y_axis='log')\n",
        "    plt.colorbar(format=\"%+2.0f dB\")\n",
        "    if title: plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def load_audio(path, sr=DEFAULT_SAMPLE_RATE):\n",
        "    \"\"\"Load mono audio to [1, samples] float32.\"\"\"\n",
        "    y, _sr = librosa.load(path, sr=sr, mono=True)\n",
        "    return y.astype(np.float32)[None, :]\n",
        "\n",
        "def reset_crepe():\n",
        "    ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Optional: quiet TensorFlow GPU warnings if you’re CPU-only\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n"
      ],
      "metadata": {
        "id": "uKuX6B72Unfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3 #\n",
        "# =================\n",
        "AUDIO_PATH = \"./audio/my_voice_3s.wav\"\n",
        "# =================\n",
        "\n",
        "# Load audio\n",
        "audio = load_audio(AUDIO_PATH, sr=DEFAULT_SAMPLE_RATE)\n",
        "\n",
        "print(\"Extracting audio features...\")\n",
        "reset_crepe()\n",
        "t0 = time.time()\n",
        "audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print(f\"Audio features took {time.time()-t0:.1f} s\")\n",
        "\n",
        "# Quick preview\n",
        "specplot(audio, title=\"Input Audio\")\n",
        "play(audio)\n"
      ],
      "metadata": {
        "id": "h3YqwCrKU6vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4 #\n",
        "import os, time, pickle, gin\n",
        "import ddsp, ddsp.training\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "assert 'audio' in globals(), \"Run your audio loading cell first.\"\n",
        "assert 'audio_features' in globals(), \"Compute audio_features first.\"\n",
        "\n",
        "def find_gin_file(model_dir):\n",
        "    cands = [f for f in os.listdir(model_dir) if f.endswith(\".gin\")]\n",
        "    if not cands:\n",
        "        raise FileNotFoundError(f\"No .gin found in {model_dir}\")\n",
        "    cands.sort()\n",
        "    return os.path.join(model_dir, cands[0])\n",
        "\n",
        "def find_ckpt_prefix(model_dir):\n",
        "    idx_files = [f for f in os.listdir(model_dir) if f.startswith(\"ckpt-\") and f.endswith(\".index\")]\n",
        "    if not idx_files:\n",
        "        raise FileNotFoundError(f\"No checkpoint .index file in {model_dir}\")\n",
        "    def step_of(n):\n",
        "        try: return int(n.split(\"-\")[1].split(\".\")[0])\n",
        "        except: return -1\n",
        "    idx_files.sort(key=step_of)\n",
        "    latest = idx_files[-1]\n",
        "    return os.path.join(model_dir, latest.rsplit(\".index\", 1)[0])\n",
        "\n",
        "gin_file = find_gin_file(MODEL_DIR)\n",
        "ckpt = find_ckpt_prefix(MODEL_DIR)\n",
        "print(\"gin:\", gin_file)\n",
        "print(\"ckpt:\", ckpt)\n",
        "\n",
        "# Optional dataset stats\n",
        "DATASET_STATS = None\n",
        "ds_stats_fp = os.path.join(MODEL_DIR, \"dataset_statistics.pkl\")\n",
        "if tf.io.gfile.exists(ds_stats_fp):\n",
        "    try:\n",
        "        with tf.io.gfile.GFile(ds_stats_fp, \"rb\") as f:\n",
        "            DATASET_STATS = pickle.load(f)\n",
        "        print(\"Loaded dataset_statistics.pkl\")\n",
        "    except Exception as e:\n",
        "        print(\"Warn: dataset stats load failed:\", e)\n",
        "\n",
        "# Parse gin and align dimensions\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
        "n_samples_train  = gin.query_parameter('Harmonic.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config([\n",
        "        f'Harmonic.n_samples = {n_samples}',\n",
        "        f'FilteredNoise.n_samples = {n_samples}',\n",
        "        f'F0LoudnessPreprocessor.time_steps = {time_steps}',\n",
        "        'oscillator_bank.use_angular_cumsum = True',\n",
        "    ])\n",
        "\n",
        "# Trim features to match shapes\n",
        "for k in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "    audio_features[k] = audio_features[k][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "# Restore and build\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "print(\"Restoring…\")\n",
        "t0 = time.time()\n",
        "model.restore(ckpt)\n",
        "_ = model(audio_features, training=False)\n",
        "print(f\"Model ready in {time.time()-t0:.1f}s\")\n"
      ],
      "metadata": {
        "id": "SDktibCpU7G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5 #\n",
        "# ==== Cell 5: Load DDSP model (server-safe, no google.colab) ====\n",
        "import os, pickle, time, gin\n",
        "import numpy as np\n",
        "import ddsp, ddsp.training\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "# We are NOT in Colab UI here, so define the constant directly\n",
        "DEFAULT_SAMPLE_RATE = 16000\n",
        "\n",
        "# Safety: ensure 'audio' & 'audio_features' from Cell 3 exist\n",
        "assert 'audio' in globals(), \"Audio not loaded. Run Cell 3 first.\"\n",
        "assert 'audio_features' in globals(), \"Audio features not computed. Run Cell 3 first.\"\n",
        "\n",
        "# 1) Pick the model directory: prefer your /violin_ae; otherwise unzip the official zip once.\n",
        "MODEL_DIR_CANDIDATES = []\n",
        "\n",
        "if os.path.isdir(\"./models/violin_ae\"):\n",
        "    MODEL_DIR_CANDIDATES.append(\"./models/violin_ae\")\n",
        "\n",
        "# If you uploaded the official zip, we can unzip (idempotent) to /ddsp_models/solo_violin_ckpt\n",
        "if os.path.exists(\"./models/solo_violin_ckpt.zip\"):\n",
        "    os.makedirs(\"./models\", exist_ok=True)\n",
        "    # Unzip only if target dir doesn't already exist\n",
        "    if not os.path.isdir(\"./models/solo_violin_ckpt\"):\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(\"./models/solo_violin_ckpt.zip\", \"r\") as zf:\n",
        "            zf.extractall(\"./models\")\n",
        "    MODEL_DIR_CANDIDATES.append(\"./models/solo_violin_ckpt\")\n",
        "\n",
        "# Choose the first directory that looks valid\n",
        "def has_minimum_files(d):\n",
        "    if not os.path.isdir(d):\n",
        "        return False\n",
        "    names = set(os.listdir(d))\n",
        "    need = {\"operative_config-0.gin\"}  # any .gin is fine; we search below anyway\n",
        "    has_gin = any(n.endswith(\".gin\") for n in names)\n",
        "    has_ckpt = any(n.startswith(\"ckpt-\") and n.endswith(\".index\") for n in names)\n",
        "    has_ckpt_data = any(n.startswith(\"ckpt-\") and n.endswith(\".data-00000-of-00001\") for n in names)\n",
        "    return has_gin and has_ckpt and has_ckpt_data\n",
        "\n",
        "MODEL_DIR = None\n",
        "for d in MODEL_DIR_CANDIDATES:\n",
        "    if has_minimum_files(d):\n",
        "        MODEL_DIR = d\n",
        "        break\n",
        "\n",
        "if MODEL_DIR is None:\n",
        "    # Helpful diagnostics\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find a valid DDSP checkpoint directory.\\n\"\n",
        "        \"Expected either ./models/violin_ae or ./models/solo_violin_ckpt (unzipped) \"\n",
        "        \"to contain: operative_config-*.gin, ckpt-XXXX.index, ckpt-XXXX.data-00000-of-00001\\n\"\n",
        "        \"Tip: verify with: !ls -la ./models/violin_ae && echo --- && !ls -la ./models\"\n",
        "    )\n",
        "\n",
        "print(f\"Using model directory: {MODEL_DIR}\")\n",
        "\n",
        "# 2) Locate gin file and checkpoint prefix\n",
        "def find_gin_file(model_dir):\n",
        "    # prefer 'operative_config-0.gin' but accept any .gin\n",
        "    cands = [f for f in os.listdir(model_dir) if f.endswith(\".gin\")]\n",
        "    if not cands:\n",
        "        raise FileNotFoundError(f\"No .gin config found in {model_dir}.\")\n",
        "    # stable pick\n",
        "    cands.sort()\n",
        "    return os.path.join(model_dir, cands[0])\n",
        "\n",
        "def find_ckpt_prefix(model_dir):\n",
        "    idx_files = [f for f in os.listdir(model_dir) if f.startswith(\"ckpt-\") and f.endswith(\".index\")]\n",
        "    if not idx_files:\n",
        "        raise FileNotFoundError(f\"No checkpoint index file found in {model_dir}.\")\n",
        "    # pick the largest step if multiple\n",
        "    def step_of(name):\n",
        "        try:\n",
        "            return int(name.split(\"-\")[1].split(\".\")[0])\n",
        "        except:\n",
        "            return -1\n",
        "    idx_files.sort(key=step_of)\n",
        "    latest = idx_files[-1]\n",
        "    base = latest.rsplit(\".index\", 1)[0]\n",
        "    return os.path.join(model_dir, base)\n",
        "\n",
        "gin_file = find_gin_file(MODEL_DIR)\n",
        "ckpt = find_ckpt_prefix(MODEL_DIR)\n",
        "print(f\"Found gin:   {gin_file}\")\n",
        "print(f\"Found ckpt:  {ckpt}\")\n",
        "\n",
        "# 3) Load dataset statistics if present (optional but improves auto-adjust later)\n",
        "DATASET_STATS = None\n",
        "ds_stats_fp = os.path.join(MODEL_DIR, \"dataset_statistics.pkl\")\n",
        "if tf.io.gfile.exists(ds_stats_fp):\n",
        "    try:\n",
        "        with tf.io.gfile.GFile(ds_stats_fp, \"rb\") as f:\n",
        "            DATASET_STATS = pickle.load(f)\n",
        "        print(\"Loaded dataset statistics.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to load dataset_statistics.pkl: {e}\")\n",
        "\n",
        "# 4) Parse gin and align dimensions with our current audio length\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "time_steps_train = gin.query_parameter('F0LoudnessPreprocessor.time_steps')\n",
        "n_samples_train  = gin.query_parameter('Harmonic.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "# Derive the length we need for this audio\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "gin_overrides = [\n",
        "    f'Harmonic.n_samples = {n_samples}',\n",
        "    f'FilteredNoise.n_samples = {n_samples}',\n",
        "    f'F0LoudnessPreprocessor.time_steps = {time_steps}',\n",
        "    'oscillator_bank.use_angular_cumsum = True',  # numerical stability\n",
        "]\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config(gin_overrides)\n",
        "\n",
        "# 5) Trim features to match the new shapes\n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "    audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "# 6) Restore model and do a dry forward to build it\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "print(\"Restoring model…\")\n",
        "t0 = time.time()\n",
        "model.restore(ckpt)\n",
        "_ = model(audio_features, training=False)  # build graph\n",
        "print(f\"Model restored and built in {time.time() - t0:.1f}s\")\n",
        "print(\"Ready for the next cell (resynthesis).\")\n"
      ],
      "metadata": {
        "id": "MqSZC5ggU7Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6 #\n",
        "# --- Safe helpers (no google.colab) ---\n",
        "import numpy as np, librosa, matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "DEFAULT_SAMPLE_RATE = 16000\n",
        "\n",
        "def play(arr_2d_or_1d, sr=DEFAULT_SAMPLE_RATE):\n",
        "    \"\"\"Play [1, N] or [N] array audio safely.\"\"\"\n",
        "    y = arr_2d_or_1d[0] if isinstance(arr_2d_or_1d, np.ndarray) and arr_2d_or_1d.ndim == 2 else arr_2d_or_1d\n",
        "    display(Audio(y, rate=sr))\n",
        "\n",
        "def specplot(x_2d_or_1d, title=None, sr=DEFAULT_SAMPLE_RATE):\n",
        "    \"\"\"Simple mel-spectrogram plot (no colab_utils).\"\"\"\n",
        "    y = x_2d_or_1d[0] if isinstance(x_2d_or_1d, np.ndarray) and x_2d_or_1d.ndim == 2 else x_2d_or_1d\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, fmax=sr//2)\n",
        "    Sdb = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(8,3))\n",
        "    librosa.display.specshow(Sdb, sr=sr, x_axis='time', y_axis='mel')\n",
        "    if title: plt.title(title)\n",
        "    plt.colorbar(format=\"%+0.1f dB\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Optional autotune: try to import, else fall back to pass-through\n",
        "try:\n",
        "    from ddsp.colab.colab_utils import auto_tune as _at, get_tuning_factor as _gtf  # may fail if google.colab is required\n",
        "    _HAVE_AT = True\n",
        "except Exception:\n",
        "    _HAVE_AT = False\n",
        "    def _at(f0_midi, tuning_factor, mask_on, amount=0.0):\n",
        "        return f0_midi  # no-op\n",
        "    def _gtf(f0_midi, f0_confidence, mask_on):\n",
        "        return 1.0\n",
        "\n",
        "from ddsp.training.postprocessing import detect_notes, fit_quantile_transform\n",
        "import ddsp\n",
        "\n",
        "# --- Params you can tweak ---\n",
        "threshold = 1.0      # note detection strength\n",
        "ADJUST = True\n",
        "quiet = 20           # reduce note-off loudness\n",
        "autotune_amt = 0.0   # 0..1; keep 0.0 unless _HAVE_AT is True\n",
        "pitch_shift_oct = 0  # integer octaves\n",
        "loudness_shift_db = 0\n",
        "\n",
        "# --- Work off a copy ---\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "def shift_ld(feats, ld_shift=0.0):\n",
        "    feats['loudness_db'] += ld_shift\n",
        "    return feats\n",
        "\n",
        "def shift_f0(feats, pitch_shift=0.0):\n",
        "    feats['f0_hz'] *= (2.0 ** pitch_shift)\n",
        "    feats['f0_hz'] = np.clip(feats['f0_hz'], 0.0, librosa.midi_to_hz(110.0))\n",
        "    return feats\n",
        "\n",
        "mask_on = None\n",
        "if ADJUST and (DATASET_STATS is not None):\n",
        "    mask_on, note_on_value = detect_notes(\n",
        "        audio_features['loudness_db'],\n",
        "        audio_features['f0_confidence'],\n",
        "        threshold\n",
        "    )\n",
        "    if np.any(mask_on):\n",
        "        # Match register\n",
        "        target_mean_pitch = DATASET_STATS['mean_pitch']\n",
        "        pitch_midi = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "        mean_pitch = np.mean(pitch_midi[mask_on])\n",
        "        p_diff = target_mean_pitch - mean_pitch\n",
        "        p_diff_oct = p_diff / 12.0\n",
        "        p_diff_oct = np.floor(p_diff_oct) if p_diff_oct > 1.5 else np.ceil(p_diff_oct)\n",
        "        audio_features_mod = shift_f0(audio_features_mod, p_diff_oct)\n",
        "\n",
        "        # Loudness quantile normalize (note-on / note-off handling)\n",
        "        _, loudness_norm = fit_quantile_transform(\n",
        "            audio_features['loudness_db'],\n",
        "            mask_on,\n",
        "            inv_quantile=DATASET_STATS['quantile_transform'])\n",
        "        mask_off = np.logical_not(mask_on)\n",
        "        loudness_norm[mask_off] -= quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
        "        loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
        "        audio_features_mod['loudness_db'] = loudness_norm\n",
        "\n",
        "        # Optional autotune\n",
        "        if autotune_amt and _HAVE_AT:\n",
        "            f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "            tuning_factor = _gtf(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "            f0_midi_at = _at(f0_midi, tuning_factor, mask_on, amount=autotune_amt)\n",
        "            audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "    else:\n",
        "        print(\"Skipping auto-adjust (no notes detected).\")\n",
        "else:\n",
        "    print(\"Skipping auto-adjust (disabled or no dataset stats).\")\n",
        "\n",
        "# Manual tweaks\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_shift_db)\n",
        "audio_features_mod = shift_f0(audio_features_mod, pitch_shift_oct)\n",
        "\n",
        "# Quick comparison plots\n",
        "TRIM = -15\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(audio_features['loudness_db'][:TRIM], label=\"orig\")\n",
        "plt.plot(audio_features_mod['loudness_db'][:TRIM], label=\"mod\")\n",
        "plt.legend(); plt.title(\"loudness_db\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]), label=\"orig\")\n",
        "plt.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]), label=\"mod\")\n",
        "plt.legend(); plt.title(\"f0 [midi]\"); plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "xV1Na51SU7tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 #\n",
        "import numpy as np, librosa, matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import tensorflow as tf\n",
        "import librosa.display as ldisplay  # <-- import once, avoid shadowing\n",
        "\n",
        "DEFAULT_SAMPLE_RATE = 16000\n",
        "\n",
        "def _to_np1d(x):\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        x = x[0]\n",
        "    if tf.is_tensor(x):\n",
        "        x = x.numpy()\n",
        "    x = np.asarray(x)\n",
        "    if x.ndim == 2 and x.shape[0] == 1:\n",
        "        x = x[0]\n",
        "    if x.ndim != 1:\n",
        "        raise ValueError(f\"Expected 1D audio, got shape {x.shape}\")\n",
        "    return x.astype(np.float32)\n",
        "\n",
        "def play(arr, sr=DEFAULT_SAMPLE_RATE):\n",
        "    y = _to_np1d(arr)\n",
        "    display(Audio(y, rate=sr))\n",
        "\n",
        "def specplot(arr, title=None, sr=DEFAULT_SAMPLE_RATE):\n",
        "    y = _to_np1d(arr)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80, fmax=sr//2)\n",
        "    Sdb = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(8,3))\n",
        "    ldisplay.specshow(Sdb, sr=sr, x_axis='time', y_axis='mel')  # use alias\n",
        "    if title: plt.title(title)\n",
        "    plt.colorbar(format=\"%+0.1f dB\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "NWio3PqpU7-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8 #\n",
        "print(\"Original:\")\n",
        "play(audio, sr=DEFAULT_SAMPLE_RATE)\n",
        "print(\"Resynthesis:\")\n",
        "play(audio_gen, sr=DEFAULT_SAMPLE_RATE)\n",
        "\n",
        "specplot(audio, title=\"Original\", sr=DEFAULT_SAMPLE_RATE)\n",
        "specplot(audio_gen, title=\"Resynthesis\", sr=DEFAULT_SAMPLE_RATE)\n",
        "\n",
        "import soundfile as sf\n",
        "OUT_WAV = \"./resynthesis_violin.wav\"\n",
        "sf.write(OUT_WAV, _to_np1d(audio_gen), DEFAULT_SAMPLE_RATE)\n",
        "print(\"Saved:\", OUT_WAV)\n"
      ],
      "metadata": {
        "id": "OVsemhj1U8OL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}